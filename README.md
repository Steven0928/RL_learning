# RL

1. #### Q-learning&DQN

   ​		Q-learning和DQN是两种典型的基于值迭代的强化学习方法，在迭代过程中采用贪心法根据当前的Q值来选择最佳的action（argmax方法）。

   ​		Q-leaning将迭代环境内agent的所有状态离散化并以此确定Q表的大小，通过ε-greedy策略确定动作，执行此次动作后，用环境返回的状态和奖励根据
   $$
   Q(s, a) = Q(s, a) + α * [r + γ *argmax(a', Q(s', a')) - Q(s, a) ]
   $$
   来更新Q表。Q-learning的优点在于算法简单且易于实现，缺点在于Q-learning本质上是一种查表遍历的方法，其处理能力取决于Q表的大小，在状态空间和动作空间较为复杂的情况下，其需要的Q表大小过于累赘。

   ​		DQN总体迭代方式上与Q-learning基本一致，其改进之处在于用深度学习的方式将巨大的Q表简化为两个神经网络：估计网络和目标网络。估计网络用于预测当前的 *状态-动作* 对对应的Q值，目标网络用于估计当前策略下一个状态的最大Q值。DQN利用储存经验回放的方式来学习更新网络。在选择当前Q值下最优动作并在环境中执行后，将当前的状态、奖励、动作、下一状态作为一组经验存入经验池。当经验池中样本达到一定数量后，随机的从经验池中提取一批经验送入网络进行学习。首先使用估计网络计算当前的 *状态-动作* 对的Q值，然后使用目标网络计算下一状态的最大Q值，并通过：
   $$
   loss = (r + γ * argmax(a', Q(s', a')) - Q(s, a)) ^2
   $$
   更新估计网络。

   ​		Q-learning和DQN这样基于值迭代的方法由于采用ε-greedy方法难以确定探索值和现有值之间的平衡，而其用argmax确定最佳action的方式，使其很难以处理连续动作空间的问题。

   

2. #### DDPG

   ​		DDPG结合了策略梯度方法（DPG）和DQN的优点，使用off-policy的actor-critic方法，让值函数来对策略函数进行评估。

   DDPG的方法主要由四个网络构成：actor网络（策略网络）；critic网络（值网络）；target_actor网络（目标策略网络）；target_critic网络（目标值网络）。与DQN类似，DDPG同样采用经验回放的方式让网络进行学习。

   在训练环境中，agent使用actor网络来确定当前的action值，由于DDPG不像DQN用argmax确定动作，而是采用策略函数来一步一步逼进下一状态的最佳动作，使DDPG算法可以很好的处理连续动作空间的问题。在确定当前的action值并执行相应动作后，类似的，将当前的经验组存入经验池，等待采样学习。critic网络和actor网络的更新依据为：
   $$
   critic_-loss = (r + γ * Q'(s', a') - Q(s, a))^2
   $$

   $$
   actor_-loss = -Q(s, a)
   $$

   为最大优化actor策略网络，将actor_loss定为负的Q值，这样，在优化过程中，为了最小化loss值，Q值的大小会被驱动着向最大化Q值发展。

   为防止策略网络和值网络的更新因为环境变化产生剧烈变化，DDPG采用软更新策略让target目标网络对进行缓慢的稳定的更新，公式如下：
   $$
   net_-target_-param = soft_-tau * net_-param + (1 - soft_-tau) * net_-target_-param
   $$
   wesoft_tau是一个非常小接近于0的超参数， 代表目标网络的更新速度。

   ​		DDPG很好的用策略逼近的方法解决了DQN难以解决连续动作空间的问题，但同样的，其经验回放的学习方法也让其和DQN一样样本效率极低，在学习过程中需要大量的交互样本。

   

   3. #### 心得

   ​		就目前学习下来，强化学习的关键在于交互环境和奖励函数的设计；交互环境决定了agent的状态空间参数，奖励函数的设计决定了网络更新发展的方向。

   ​		奖励函数的设计应该为阶段性的，应为一个小目标一个小目标的实现，在实现一个阶段性目标并稳定之后再对奖励函数的设计进行相应的修改，让agent在完成当前目标的情况下向更进一步的目标发展。奖励函数如果设计过于的复杂，很容易让agent进入“摆烂”的状态。可以参考[AI Learns to Walk (deep reinforcement learning) - YouTube](https://www.youtube.com/watch?v=L_4BPjLBF4E)一个阶段一个阶段的去实现某个目标，能让agent更稳定更准确的发展。
